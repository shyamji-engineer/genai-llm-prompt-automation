# LLM Prompt and Automation Experiments

An introductory yet hands-on **Generative AI experimentation project** built with Python.
This project demonstrates how to design **prompt-based workflows**, integrate **multiple LLM backends**, build **LangChain pipelines**, and create **automation scripts** for real-world text processing tasks.

The system follows a **local-first, modular architecture**, with optional cloud LLM support.

---

## ğŸš€ Project Goals

- Learn and implement **prompt engineering**
- Experiment with **local and cloud-based LLMs**
- Build **LangChain pipelines** for structured GenAI workflows
- Automate repetitive text-processing tasks using LLMs
- Explore **semantic embeddings and similarity**
- Design a **clean, modular GenAI project structure**

---

## ğŸ§  Key Features

- Prompt-driven **text generation, summarization, and analysis**
- **Local LLM inference** using Ollama (offline & reliable)
- **Cloud LLM support** using Google Gemini API (optional)
- **Hugging Face local models** for text generation and embeddings
- **LangChain pipelines** connecting prompts â†’ models â†’ outputs
- **Automation scripts** for file-based text workflows
- **Semantic similarity** using embeddings (PyTorch + Transformers)
- **Interactive runner** for model and input selection

---

## ğŸ—‚ï¸ Project Structure

```
llm_prompt_automation/
â”‚
â”œâ”€â”€ runner.py                  # Interactive demo runner (optional)
â”œâ”€â”€ main.py                    # Simple demo entry point
â”œâ”€â”€ config.py                  # Environment & configuration
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ prompts/                   # Prompt templates
â”‚   â”œâ”€â”€ text_generation.txt
â”‚   â”œâ”€â”€ summarization.txt
â”‚   â””â”€â”€ analysis.txt
â”‚
â”œâ”€â”€ pipelines/                 # LangChain pipelines
â”‚   â”œâ”€â”€ text_generation_chain.py
â”‚   â”œâ”€â”€ summarization_chain.py
â”‚   â”œâ”€â”€ analysis_chain.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ llm/                       # LLM backends
â”‚   â”œâ”€â”€ ollama_client.py       # Local LLM (default)
â”‚   â”œâ”€â”€ gemini_client.py       # Cloud LLM (optional)
â”‚   â””â”€â”€ hf_client.py           # Hugging Face local models
â”‚
â”œâ”€â”€ automation/                # LLM-powered automation
â”‚   â”œâ”€â”€ auto_summarize.py
â”‚   â””â”€â”€ auto_analyze.py
â”‚
â”œâ”€â”€ embeddings/                # Semantic similarity
â”‚   â”œâ”€â”€ embedding_model.py
â”‚   â””â”€â”€ similarity.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ file_utils.py          # File reading utilities
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_text.txt
â”‚
â””â”€â”€ outputs/
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create virtual environment (optional but recommended)
```bash
python -m venv .venv
source .venv/bin/activate
```

### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure environment variables

Create a `.env` file:

```env
# Local LLM
OLLAMA_MODEL=deepseek-r1:1.5b

# Cloud LLM (optional)
GEMINI_API_KEY=your_gemini_api_key_here

# Hugging Face (local usage only)
HF_API_TOKEN=optional
TRANSFORMERS_NO_TF=1
```

> âš ï¸ The project runs **fully offline** using Ollama and Hugging Face local models.  
> Gemini is optional and used only when explicitly selected.

---

## â–¶ï¸ How to Run

### ğŸ”¹ Interactive Runner (Recommended)
```bash
python runner.py
```

You can:
1. Select LLM backend  
   - Ollama (local)  
   - Gemini (cloud)  
   - Hugging Face (local)
2. Select input type  
   - Manual text  
   - File input

---

### ğŸ”¹ Run Individual Components

#### Text generation / summarization demo
```bash
python main.py
```

#### Automation scripts
```bash
python -m automation.auto_summarize
python -m automation.auto_analyze
```

#### Embeddings similarity
```bash
python -m embeddings.similarity
```

---

## ğŸ§© LLM Backends Explained

### ğŸŸ¢ Ollama (Local â€“ Default)
- Fully offline
- Reliable
- No API cost
- Used in main pipelines

### ğŸŸ¡ Gemini (Cloud â€“ Optional)
- Fast cloud-based LLM
- Requires API key
- Used only when selected explicitly

### ğŸŸ¢ Hugging Face (Local)
- `google/flan-t5-small` for instruction-based text generation (<1GB)
- Hugging Face Transformers for embeddings
- No cloud inference dependency

---

## ğŸ” Embeddings & Similarity

- Text converted into dense vectors using Transformer models
- Cosine similarity used to measure semantic closeness
- Demonstrates core NLP understanding beyond text generation

---

## ğŸ—ï¸ Architecture Diagram

```
User Input (Manual / File)
        â”‚
        â–¼
 Interactive Runner (runner.py)
        â”‚
        â”œâ”€â”€ Ollama (Local LLM)
        â”‚       â””â”€â”€ LangChain Pipelines
        â”‚
        â”œâ”€â”€ Gemini (Cloud LLM)
        â”‚       â””â”€â”€ Direct API Call
        â”‚
        â””â”€â”€ Hugging Face (Local Models)
                â”œâ”€â”€ Text Generation (FLAN-T5)
                â””â”€â”€ Embeddings & Similarity
        â”‚
        â–¼
 Generated Output / Analysis / Summary
```

This diagram shows how user input flows through the interactive runner, reaches the selected LLM backend, and returns the generated output in a modular and decoupled manner.

---

## ğŸ“¸ Screenshots / Demo

Below are real screenshots captured during development and testing. These demonstrate the interactive runner, model selection, automation workflows, and embeddings execution.

> **Note:** For better repository hygiene, it is recommended to place screenshots inside an `assets/screenshots/` directory and rename them meaningfully as shown below.

### 1ï¸âƒ£ Interactive Runner â€“ Model Selection
![Runner Model Selection](assets/screenshots/runner_model_selection.png)

*(Screenshot shows interactive LLM model selection in `runner.py`)*

---


### 2ï¸âƒ£ Input Type Selection (Manual / File)
![Input Type Selection](assets/screenshots/input_type_selection.png)

*(Screenshot shows manual vs file input selection flow)*

---


### 3ï¸âƒ£ Gemini Cloud Model â€“ Text Generation Output
![Gemini Output](assets/screenshots/gemini_output.png)

*(Screenshot shows successful LLM-generated response using **Google Gemini (cloud)** model via the interactive runner)*

---



### 4ï¸âƒ£ Automation Script â€“ Summarization Output Saved
![Automation Summarize](assets/screenshots/automation_summarize.png)

*(Screenshot shows automated summarization output saved to file)*

---


### 5ï¸âƒ£ Embeddings Similarity Execution
![Embeddings Similarity](assets/screenshots/automation_and_embeddings_similarity.png)

*(Screenshot shows embeddings similarity calculation and automation execution)*

These screenshots validate that all major components of the project are functional and integrated correctly.

---


## ğŸ§  Design Philosophy

- **Modular by design**: each feature works independently
- **Local-first approach**: avoids API instability and billing risks
- **Separation of concerns**: prompts, pipelines, LLMs, and automation are isolated
- **Experiment-focused**: built for learning, not production deployment

---

## ğŸ“Œ Limitations (Intentional)

- No vector database (FAISS / Pinecone)
- No RAG or agent frameworks
- No production deployment setup

These are intentionally excluded to keep the project focused on **GenAI fundamentals**.

---

## ğŸ¯ Learning Outcomes

- Practical understanding of prompt engineering
- Hands-on LangChain pipeline creation
- Experience with local vs cloud LLM trade-offs
- Exposure to real-world GenAI debugging
- Clean GenAI project structuring

---

## ğŸ·ï¸ Resume Description (Safe to Use)

> Built a modular GenAI experimentation system using Python and LangChain, implementing prompt-based text generation, summarization, and analysis workflows. Integrated local LLM inference via Ollama, optional cloud-based inference using Google Gemini, and Hugging Face local models for text generation and embedding-based semantic similarity. Developed LLM-powered automation scripts for text processing.

---

## ğŸ‘¤ Author

**Shyam Ji**  
AI Engineer | GenAI Enthusiast | Data Science | Machine Learning | Python Developer | Cyber Security Analyst & Ethical Hacker

- GitHub: https://github.com/shyamji-engineer
- Portfolio: https://shyamjirajput.vercel.app/
- LinkedIn: https://www.linkedin.com/in/shyamjiengineer
- Location: India

---

## ğŸ“œ License

This project is intended for **learning and experimentation purposes**.

